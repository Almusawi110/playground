{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Code is mostly re-used from https://github.com/moduIo/Deep-Q-network/blob/master/DQN.ipynb\n",
    "# Code may change with more clean-ups and explanation\n",
    "import gym\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dqn_agent import DQN_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import blend_images, process_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Work\\Conestoga\\cscn_venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Work\\Conestoga\\cscn_venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 27, 20, 32)        2080      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 27, 20, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 10, 64)        32832     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 14, 10, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 10, 64)        36928     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 14, 10, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8960)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               4588032   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4661924 (17.78 MB)\n",
      "Trainable params: 4661924 (17.78 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v4', render_mode=\"rgb_array\")\n",
    "state_size = (105, 80, 1)\n",
    "action_size = env.action_space.n\n",
    "minimum_memory_size = 1000 # Minimum experience replay memory size before training\n",
    "max_memory_size = 10000 # Maximum experience replay memory\n",
    "agent = DQN_Agent(state_size, action_size, memory_size=max_memory_size)\n",
    "\n",
    "episodes = 50\n",
    "batch_size = 64\n",
    "skip_start = 90  # Breakout-v0 waits for 90 actions before the episode begins\n",
    "total_time = 0   # Counter for total number of steps taken\n",
    "all_rewards = 0  # Used to compute avg reward over time\n",
    "blend = 4        # Number of images to blend\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create tensorboard logs (plots)\n",
    "import datetime\n",
    "import tensorflow.summary as summary_writer\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "train_log_dir = './logs/' + current_time + '/train'\n",
    "train_summary_writer = summary_writer.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "## Visualize state\n",
    "observation = env.reset()\n",
    "observation = env.step(1)\n",
    "for skip in range(2): # skip the start of each game/\n",
    "    observation = env.step(0)\n",
    "\n",
    "# observation = observation[100::]\n",
    "processed_observation = process_frame(observation[0])\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def show_state(state, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(state)\n",
    "    # plt.title(\"Step: %d %s\" % (step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "show_state(processed_observation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cropped_obs = processed_observation[0][25:,:]\n",
    "# cropped_obs = np.expand_dims(cropped_obs.reshape(105, 80, 1), axis=0)\n",
    "# show_state(cropped_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blend = 4\n",
    "max_steps_per_episode = 2000\n",
    "for e in range(episodes):\n",
    "    total_reward = 0\n",
    "    game_score = 0\n",
    "    observation = env.reset()\n",
    "    # print(observation)\n",
    "    state = process_frame(observation[0])\n",
    "    images = deque(maxlen=blend)  # Array of images to be blended\n",
    "    images.append(state)\n",
    "    \n",
    "    for skip in range(skip_start): # skip the start of each game\n",
    "        env.step(0)\n",
    "    \n",
    "    # generate an episode\n",
    "    for time in range(max_steps_per_episode):\n",
    "        total_time += 1\n",
    "        \n",
    "        # Every update_rate timesteps we update the target network parameters\n",
    "        if total_time % agent.update_rate == 0:\n",
    "            # Update the target model by copying weights from Qnetwork\n",
    "            agent.update_target_model()\n",
    "        \n",
    "        # Return the avg of the last 4 frames\n",
    "        state = blend_images(images, state_size, blend)\n",
    "        \n",
    "        # Choose and apply action\n",
    "        action = agent.act(state)\n",
    "        next_observation, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        # TODO: Process the frame and save it to memory\n",
    "        # 1. pre-process the image (grayscale conversion, crop, resize...)\n",
    "        processed_state = process_frame(next_observation)\n",
    "        # 2. Combine with previous images in the image circular buffer (size=4)\n",
    "        images.append(processed_state)\n",
    "        next_state = blend_images(images, state_size, blend)\n",
    "        # 3. Add to the replay memory\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Update state, and rewards\n",
    "        state = next_state\n",
    "        game_score += reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            all_rewards += game_score\n",
    "            \n",
    "            print(\"episode: {}/{}, game score: {}, reward: {}, avg reward: {}, time: {}, total time: {}\"\n",
    "                  .format(e+1, episodes, game_score, total_reward, all_rewards/(e+1), time, total_time))   \n",
    "            break\n",
    "\n",
    "        if len(agent.memory) > minimum_memory_size and (total_time % 500) == 0:\n",
    "            # train\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "    with train_summary_writer.as_default():       \n",
    "        summary_writer.scalar('game_score', game_score, step=e)\n",
    "    # TODO: Save model every n (try 10 or 25) episodes\n",
    "    if e % 10 == 0:\n",
    "        fname = f'models/{max_memory_size}-memory_{e}-games'\n",
    "        agent.save(fname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# agent.save('models/5k-memory_100-games')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "!pip install IPython\n",
    "!pip install matplotlib\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render())\n",
    "    plt.title(\"Step: %d %s\" % (step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "game_score = 0\n",
    "total_reward = 0\n",
    "done = False\n",
    "reward = 0\n",
    "env.reset()\n",
    "images = deque(maxlen=blend)\n",
    "for t in range(2000):\n",
    "    for skip in range(skip_start): # skip the start of each game\n",
    "        env.step(0)\n",
    "    show_state(env, t)\n",
    "    total_time += 1\n",
    "    \n",
    "    # Return the avg of the last 4 frames\n",
    "    state = blend_images(images, state_size, blend)\n",
    "    \n",
    "    # Transition Dynamics\n",
    "    action = agent.greedy_act(state)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "    # Return the avg of the last 4 frames\n",
    "    next_state = process_frame(next_state)\n",
    "    images.append(next_state)\n",
    "    next_state = blend_images(images, state_size, blend)\n",
    "        \n",
    "    state = next_state\n",
    "    game_score += reward\n",
    "    reward -= 1  # Punish behavior which does not accumulate reward\n",
    "    total_reward += reward\n",
    "    time.sleep(0.05)\n",
    "    if done:\n",
    "        all_rewards += game_score\n",
    "        \n",
    "        print(\"game score: {}, reward: {}\"\n",
    "                .format(game_score, total_reward))\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
